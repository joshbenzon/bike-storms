import requests
import zipfile
import os
import pandas as pd
from bs4 import BeautifulSoup

# Define URL
data_url = 'https://s3.amazonaws.com/tripdata/'

# Load webpage content into a BeautifulSoup object
response = requests.get(data_url)
soup = BeautifulSoup(response.text, 'xml') # use soup to store xml data

# Extract and store filenames of all data files
file_list = soup.find_all('Key')
file_names = [file.get_text() for file in file_list[:-1]]

for file in file_names:
    file_url = data_url + file # each file's url
    with open(file.split('/')[-1], "wb") as file_output:
        response = requests.get(file_url)
        file_output.write(response.content)
    with zipfile.ZipFile(file.split('/')[-1], "r") as zip_ref: # unzip downloaded data files
        zip_ref.extractall("data/bike_data")        
    os.remove(file.split('/')[-1]) # remove zip file

data_directory = 'data/bike_data'
for file in os.listdir(data_directory):
    filename = os.fsdecode(file)
    if filename.endswith('.csv'): # if file is a csv
        new_filename = filename.replace(' ','').lower().split('ci', 1)[0].strip('-').replace('-','_') # add file
        os.rename(os.path.join(data_directory, filename), os.path.join(data_directory, new_filename + '.csv'))

# Load data into dataframes for easier access
dfs_dict = {}
for file in os.listdir(data_directory):
    filename = os.fsdecode(file)
    if filename.endswith('.csv'): # if file is a csv
        dfs_dict[filename.split('.')[0]] = pd.read_csv(os.path.join(data_directory, filename))
